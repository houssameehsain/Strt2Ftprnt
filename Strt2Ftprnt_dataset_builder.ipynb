{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strt2Ftprnt dataset builder\n",
    "\n",
    "*from street networks to predicted building footprint layouts.*\n",
    "\n",
    "## U-Net Architecture for Image Segmentation.\n",
    "\n",
    "*The dataset is gathered from OSM data using the OSMnx package.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "import folium\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = ox.geocode_to_gdf('Ankara, Turkey')\n",
    "ax = ox.project_gdf(city).plot()\n",
    "_ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ank = ox.graph_from_place('Ankara, Turkey', network_type='all')\n",
    "ox.plot_graph(Ank, bgcolor='#FFFFFF', node_size=0, edge_color='#111111', edge_linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_settings = dict(\n",
    "    dist=200,\n",
    "    edge_color='#000000',\n",
    "    bgcolor='#FFFFFF',\n",
    "    dpi = 300,\n",
    "    point = (39.890591,32.783029),\n",
    "    default_width=2,\n",
    "    )\n",
    "\n",
    "fig, ax = ox.plot_figure_ground(network_type='all', figsize=(8, 8), \n",
    "                                **map_settings)\n",
    "fig.savefig('D:/AI in Urban Design/DL UD dir/Ankara_trainY/imgX_test.png', dpi=map_settings['dpi']) \n",
    "\n",
    "gdf_bldings = ox.geometries.geometries_from_point(center_point=map_settings['point'], \n",
    "                                                  tags = {'building':True}, dist=map_settings['dist'])\n",
    "\n",
    "fig, ax = ox.plot.plot_footprints(gdf_bldings, ax=ax, figsize=(8, 8), \n",
    "                                  color='#000000', alpha=None, bgcolor='#FFFFFF',\n",
    "                                  bbox=ox.utils_geo.bbox_from_point(map_settings['point'], dist=map_settings['dist'], project_utm=False, return_crs=False),\n",
    "                                  save=True, show=True, close=False, \n",
    "                                  filepath='D:/AI in Urban Design/DL UD dir/Ankara_trainY/imgY_test.png',\n",
    "                                  dpi=map_settings['dpi'])\n",
    "\n",
    "coslat = np.cos(np.cos(map_settings['point'][1] / 180. * np.pi) )\n",
    "ax.set_aspect(1/coslat)\n",
    "fig.set_figwidth(10)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over multiple locations in Ankara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "north = 40.000000\n",
    "south = 39.836170\n",
    "east = 32.929667\n",
    "west = 32.630090\n",
    "\n",
    "step = 0.003\n",
    "\n",
    "diffv = north-south\n",
    "rv = int((diffv)/step)\n",
    "print(diffv, rv)\n",
    "\n",
    "diffh = east-west\n",
    "rh = int((diffh)/step)\n",
    "print(diffh, rh)\n",
    "\n",
    "count = rv*rh\n",
    "print('Expected img count: ' + str(count))\n",
    "\n",
    "vlist = []\n",
    "for i in range(rv):\n",
    "    v = north-i*step\n",
    "    vlist.append(v)\n",
    "    \n",
    "hlist = []\n",
    "for i in range(rh):\n",
    "    h = east-i*step\n",
    "    hlist.append(h)\n",
    "    \n",
    "print(vlist)\n",
    "print(hlist)\n",
    "# print(str(3966/99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "for i in vlist:\n",
    "    for j in hlist:\n",
    "        try:\n",
    "            map_settings = dict(\n",
    "                            dist=200,\n",
    "                            edge_color='#000000',\n",
    "                            bgcolor='#FFFFFF',\n",
    "                            dpi = 300,\n",
    "                            point = (i,j),\n",
    "                            default_width=2,\n",
    "                            )\n",
    "\n",
    "            fig, ax = ox.plot_figure_ground(network_type='all', figsize=(8, 8), \n",
    "                                            **map_settings)\n",
    "            fig.savefig('D:/AI in Urban Design/DL UD dir/Ankara_trainX/img_X_' + str(a) + '.png', dpi=map_settings['dpi'])\n",
    "\n",
    "            gdf_bldings = ox.geometries.geometries_from_point(center_point=map_settings['point'], \n",
    "                                                              tags = {'building':True}, dist=map_settings['dist'])\n",
    "\n",
    "            fig, ax = ox.plot.plot_footprints(gdf_bldings, ax=ax, figsize=(8, 8), \n",
    "                                              color='#000000', alpha=None, bgcolor='#FFFFFF',\n",
    "                                              bbox=ox.utils_geo.bbox_from_point(map_settings['point'], dist=map_settings['dist'], project_utm=False, return_crs=False),\n",
    "                                              save=True, show=True, close=False, \n",
    "                                              filepath='D:/AI in Urban Design/DL UD dir/Ankara_trainY/img_Y_' + str(a) + '.png',\n",
    "                                              dpi=map_settings['dpi'])\n",
    "            a += 1\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a lot more data from locations all around the world ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LOCATION COORDINATES\n",
    "north = 53.568016\n",
    "south = 53.541042\n",
    "east = 10.115901\n",
    "west = 9.922825\n",
    "\n",
    "step = 0.003\n",
    "\n",
    "diffv = north-south\n",
    "rv = int((diffv)/step)\n",
    "print(diffv, rv)\n",
    "\n",
    "diffh = east-west\n",
    "rh = int((diffh)/step)\n",
    "print(diffh, rh)\n",
    "\n",
    "count = rv*rh\n",
    "print('Expected img count: ' + str(count))\n",
    "\n",
    "vlist = []\n",
    "for i in range(rv):\n",
    "    v = north-i*step\n",
    "    vlist.append(v)\n",
    "    \n",
    "hlist = []\n",
    "for i in range(rh):\n",
    "    h = east-i*step\n",
    "    hlist.append(h)\n",
    "    \n",
    "print(vlist)\n",
    "print(hlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 13647 Extra starts here. 13856. 14096.\n",
    "# 2191 + 3498\n",
    "a = 21188\n",
    "for i in vlist:\n",
    "    for j in hlist:\n",
    "        try:\n",
    "            map_settings = dict(\n",
    "                            dist=200,\n",
    "                            edge_color='#000000',\n",
    "                            bgcolor='#FFFFFF',\n",
    "                            dpi = 300,\n",
    "                            point = (i,j),\n",
    "                            default_width=2,\n",
    "                            )\n",
    "\n",
    "            fig, ax = ox.plot_figure_ground(network_type='all', figsize=(8, 8), \n",
    "                                            **map_settings)\n",
    "            fig.savefig('D:/AI in Urban Design/DL UD dir/Extra_X/img_X_' + str(a) + '.png', dpi=map_settings['dpi'])\n",
    "\n",
    "            gdf_bldings = ox.geometries.geometries_from_point(center_point=map_settings['point'], \n",
    "                                                              tags = {'building':True}, dist=map_settings['dist'])\n",
    "\n",
    "            fig, ax = ox.plot.plot_footprints(gdf_bldings, ax=ax, figsize=(8, 8), \n",
    "                                              color='#000000', alpha=None, bgcolor='#FFFFFF',\n",
    "                                              bbox=ox.utils_geo.bbox_from_point(map_settings['point'], dist=map_settings['dist'], project_utm=False, return_crs=False),\n",
    "                                              save=True, show=True, close=False, \n",
    "                                              filepath='D:/AI in Urban Design/DL UD dir/Extra_Y/img_Y_' + str(a) + '.png',\n",
    "                                              dpi=map_settings['dpi'])\n",
    "            a += 1\n",
    "        except ValueError as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "# raw data directories\n",
    "X_img_path = \"D:/AI in Urban Design/DL UD dir/strtTOftprnt_X\"\n",
    "Y_img_path = \"D:/AI in Urban Design/DL UD dir/strtTOftprnt_Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 34728\n",
    "for i in tqdm(sorted(os.listdir(X_img_path)), ncols=100, disable=False):\n",
    "    a += 1\n",
    "    path = os.path.join(X_img_path, i)\n",
    "    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    flipped_img = cv2.flip(im, 1)\n",
    "    cv2.imwrite('D:/AI in Urban Design/DL UD dir/Extra_X/img_X_' \n",
    "                + str(a) + '.png', flipped_img)\n",
    "    \n",
    "b = 34728\n",
    "for i in tqdm(sorted(os.listdir(Y_img_path)), ncols=100, disable=False):\n",
    "    b += 1\n",
    "    path = os.path.join(Y_img_path, i)\n",
    "    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    flipped_img = cv2.flip(im, 1)\n",
    "    cv2.imwrite('D:/AI in Urban Design/DL UD dir/Extra_Y/img_Y_' \n",
    "                + str(b) + '.png', flipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = os.listdir(X_img_path)\n",
    "lsorted = l.sort()\n",
    "print(l)\n",
    "print(lsorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in tqdm(sorted(os.listdir(X_img_path)), ncols=100, disable=False):\n",
    "    a += 1\n",
    "    path = os.path.join(X_img_path, i)\n",
    "    im = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    y,x = im[:,:,3].nonzero() # get the nonzero alpha coordinates\n",
    "    minx = np.min(x)\n",
    "    miny = np.min(y)\n",
    "    maxx = np.max(x)\n",
    "    maxy = np.max(y)\n",
    "    cropImg = im[miny:maxy, minx:maxx]\n",
    "    cv2.imwrite('D:/AI in Urban Design/DL UD dir/Extra_Xcrop/img_X_' \n",
    "                + str(int(re.search(r'\\d+', i).group())) + '.png', cropImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(X_img_path, Y_img_path, img_size):\n",
    "    \n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "\n",
    "    X_img_count = 0\n",
    "    Y_img_count = 0\n",
    "    \n",
    "    for i in tqdm(os.listdir(X_img_path), ncols=100, disable=False):\n",
    "        path = os.path.join(X_img_path, i)\n",
    "        X = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        X = cv2.resize(X, (img_size, img_size), interpolation = cv2.INTER_AREA)\n",
    "        X = np.array(X)\n",
    "        X = X.reshape((1, img_size, img_size))\n",
    "        X = X / 255\n",
    "        X = 1 - X\n",
    "        X_data.append(X)\n",
    "        X_img_count += 1\n",
    "\n",
    "    for i in tqdm(os.listdir(Y_img_path), ncols=100, disable=False):\n",
    "        path = os.path.join(Y_img_path, i)\n",
    "        Y = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        Y = cv2.resize(Y, (img_size, img_size), interpolation = cv2.INTER_AREA)\n",
    "        Y = np.array(Y)\n",
    "        Y = Y.reshape((1, img_size, img_size))\n",
    "        Y = Y / 255\n",
    "        Y = 1 - Y\n",
    "        Y_data.append(Y)\n",
    "        Y_img_count += 1    \n",
    "            \n",
    "    print('X Image_count:' + str(X_img_count))\n",
    "    print('Y Image_count:' + str(Y_img_count))\n",
    "        \n",
    "    # train, val, test : 0.6, 0.2, 0.2 Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "    \n",
    "    print('X_train Image_count:' + str(len(X_train))) \n",
    "    print('Y_train Image_count:' + str(len(Y_train)))\n",
    "    print('X_val Image_count:' + str(len(X_val))) \n",
    "    print('Y_val Image_count:' + str(len(Y_val)))\n",
    "    print('X_test Image_count:' + str(len(X_test))) \n",
    "    print('Y_test Image_count:' + str(len(Y_test)))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = make_data(X_img_path, Y_img_path, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing images\n",
    "index = 1345\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(X_train[index].reshape((img_size, img_size)), cmap='gray')\n",
    "ax.set_title('Input Sample')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(Y_train[index].reshape((img_size, img_size)), cmap='gray')\n",
    "ax.set_title('Ground Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1345\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow((1 - X_train[index]).reshape((img_size, img_size)), cmap='gray')\n",
    "ax.set_title('Input Sample')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow((1 - Y_train[index]).reshape((img_size, img_size)), cmap='gray')\n",
    "ax.set_title('Ground Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the data\n",
    "print('Training data shape is: ' + str(X_train[0].shape))\n",
    "print('Validation data shape is: ' + str(X_val[0].shape))\n",
    "print('Test data shape is: ' + str(X_test[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataSet(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 inputs: list,\n",
    "                 targets: list,\n",
    "                 transform=None\n",
    "                 ):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.inputs_dtype = torch.float32\n",
    "        self.targets_dtype = torch.float32\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        # Select the sample\n",
    "        input_ID = self.inputs[index]\n",
    "        target_ID = self.targets[index]\n",
    "\n",
    "        # Load input and target\n",
    "        x, y = input_ID, target_ID\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.transform is not None:\n",
    "            x, y = self.transform(x, y)\n",
    "\n",
    "        # Typecasting\n",
    "        x, y = torch.from_numpy(x).type(self.inputs_dtype), torch.from_numpy(y).type(self.targets_dtype)\n",
    "        # y = torch.squeeze(y)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = SegmentationDataSet(inputs = X_train, targets = Y_train) \n",
    "\n",
    "training_dataloader = data.DataLoader(dataset=training_dataset, batch_size = 2, shuffle=True)\n",
    "x, y = next(iter(training_dataloader))\n",
    "\n",
    "print(f'x = shape: {x.shape}; type: {x.dtype}')\n",
    "print(f'x = min: {x.min()}; max: {x.max()}')\n",
    "print(f'y = shape: {y.shape}; type: {y.dtype}')\n",
    "print(f'y = min: {y.min()}; max: {y.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SegmentationDataSet(inputs = X_val, targets = Y_val)\n",
    "\n",
    "val_dataloader = data.DataLoader(dataset=val_dataset, batch_size = 2, shuffle=False)\n",
    "x, y = next(iter(val_dataloader))\n",
    "\n",
    "print(f'x = shape: {x.shape}; type: {x.dtype}')\n",
    "print(f'x = min: {x.min()}; max: {x.max()}')\n",
    "print(f'y = shape: {y.shape}; type: {y.dtype}')\n",
    "print(f'y = min: {y.min()}; max: {y.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(training_dataloader))\n",
    "print(x[0, :, :, :])\n",
    "print(y[0, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_c, out_c, seperable=True):\n",
    "    if seperable:\n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=1, bias=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, groups=out_c, bias=True),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=1, bias=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, groups=out_c, bias=True),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return conv\n",
    "    else:\n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(tensor, target_tensor):\n",
    "    target_size = target_tensor.size()[2]\n",
    "    tensor_size = tensor.size()[2]\n",
    "    delta = tensor_size - target_size\n",
    "    delta = delta // 2\n",
    "    return tensor[:, :, delta:tensor_size - delta, delta:tensor_size - delta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down_conv1 = double_conv(1, 8, seperable=False)\n",
    "        self.down_conv2 = double_conv(8, 16, seperable=True)\n",
    "        self.down_conv3 = double_conv(16, 32, seperable=True)\n",
    "        self.down_conv4 = double_conv(32, 64, seperable=True)\n",
    "        self.down_conv5 = double_conv(64, 128, seperable=True)\n",
    "        self.down_conv6 = double_conv(128, 256, seperable=True)\n",
    "        self.down_conv7 = double_conv(256, 512, seperable=True)\n",
    "        self.down_conv8 = double_conv(512, 1024, seperable=True)\n",
    "        self.down_conv9 = double_conv(1024, 2048, seperable=True)\n",
    "\n",
    "        self.up_trans1 = nn.ConvTranspose2d(2048, 1024, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv1 = double_conv(2048, 1024, seperable=True)\n",
    "        self.up_trans2 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv2 = double_conv(1024, 512, seperable=True)\n",
    "        self.up_trans3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv3 = double_conv(512, 256, seperable=True)\n",
    "        self.up_trans4 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv4 = double_conv(256, 128, seperable=False)\n",
    "        self.up_trans5 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv5 = double_conv(128, 64, seperable=False)\n",
    "        self.up_trans6 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv6 = double_conv(64, 32, seperable=False)\n",
    "        self.up_trans7 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv7 = double_conv(32, 16, seperable=False)\n",
    "        self.up_trans8 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2, bias=False)\n",
    "        self.up_conv8 = double_conv(16, 8, seperable=False)\n",
    "        \n",
    "        self.num_classes = 1\n",
    "        self.out = nn.Conv2d(8, self.num_classes, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Down\n",
    "        x1 = self.down_conv1(image)\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        x3 = self.down_conv2(x2)\n",
    "        x4 = self.max_pool_2x2(x3)\n",
    "        x5 = self.down_conv3(x4)\n",
    "        x6 = self.max_pool_2x2(x5)\n",
    "        x7 = self.down_conv4(x6)\n",
    "        x8 = self.max_pool_2x2(x7)\n",
    "        x9 = self.down_conv5(x8)\n",
    "        x10 = self.max_pool_2x2(x9)\n",
    "        x11 = self.down_conv6(x10)\n",
    "        x12 = self.max_pool_2x2(x11)\n",
    "        x13 = self.down_conv7(x12)\n",
    "        x14 = self.max_pool_2x2(x13)\n",
    "        x15 = self.down_conv8(x14)\n",
    "        x16 = self.max_pool_2x2(x15)\n",
    "        x17 = self.down_conv9(x16)\n",
    "        \n",
    "        # Up\n",
    "        x_U = self.up_trans1(x17)\n",
    "        y = crop_img(x15, x_U)\n",
    "        x_U = self.up_conv1(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans2(x_U)\n",
    "        y = crop_img(x13, x_U)\n",
    "        x_U = self.up_conv2(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans3(x_U)\n",
    "        y = crop_img(x11, x_U)\n",
    "        x_U = self.up_conv3(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans4(x_U)\n",
    "        y = crop_img(x9, x_U)\n",
    "        x_U = self.up_conv4(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans5(x_U)\n",
    "        y = crop_img(x7, x_U)\n",
    "        x_U = self.up_conv5(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans6(x_U)\n",
    "        y = crop_img(x5, x_U)\n",
    "        x_U = self.up_conv6(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans7(x_U)\n",
    "        y = crop_img(x3, x_U)\n",
    "        x_U = self.up_conv7(torch.cat([x_U, y], 1))\n",
    "        x_U = self.up_trans8(x_U)\n",
    "        y = crop_img(x1, x_U)\n",
    "        x_U = self.up_conv8(torch.cat([x_U, y], 1))\n",
    "        \n",
    "        # U-Net Output\n",
    "        x_U = self.out(x_U)\n",
    "\n",
    "        # Sigmoid layer\n",
    "        x_sig = self.sigmoid(x_U)\n",
    "        return x_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid U-net model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_block(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_block(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c)\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_block(out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=7, padding=3, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=7, padding=3, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c)\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_block(out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=7, padding=3, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c)\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_block(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=7, padding=3, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=7, padding=3, stride=1, bias=True),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "    )\n",
    "    return conv\n",
    "def sigmoid_block(out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(out_c, 1, kernel_size=1, padding=0, stride=1, bias=True),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return conv\n",
    "def conv_res(in_c, out_c):\n",
    "    resconv = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0, stride=1, bias=True)\n",
    "    return resconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid_Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid_Unet, self).__init__()\n",
    "        self.initial = initial_block(1, 32)\n",
    "        self.encblock1 = enc_block(32, 64)\n",
    "        self.encblock2 = enc_block(64, 128)\n",
    "        self.encblock3 = enc_block(128, 256)\n",
    "        self.mid = mid_block(256)\n",
    "        self.decblock1 = dec_block(128)\n",
    "        self.decblock2 = dec_block(64)\n",
    "        self.decblock3 = dec_block(32)\n",
    "        self.end = end_block(16, 16)\n",
    "        self.sigmoid = sigmoid_block(16)\n",
    "\n",
    "        self.transpose1 = nn.ConvTranspose2d(512, 128, kernel_size=2, stride=2, bias=False)\n",
    "        self.transpose2 = nn.ConvTranspose2d(256, 64, kernel_size=2, stride=2, bias=False)\n",
    "        self.transpose3 = nn.ConvTranspose2d(128, 32, kernel_size=2, stride=2, bias=False)\n",
    "        self.transpose4 = nn.ConvTranspose2d(64, 16, kernel_size=2, stride=2, bias=False)\n",
    "\n",
    "        self.res1 = conv_res(1, 32)\n",
    "        self.res2 = conv_res(32, 64)\n",
    "        self.res3 = conv_res(64, 128)\n",
    "        self.res4 = conv_res(128, 256)\n",
    "        self.res5 = conv_res(256, 256)\n",
    "        self.res6 = conv_res(128, 128)\n",
    "        self.res7 = conv_res(64, 64)\n",
    "        self.res8 = conv_res(32, 32)\n",
    "        self.res9 = conv_res(16, 16)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        x1 = self.initial(image)\n",
    "        x1res = self.relu(x1 + self.res1(image))\n",
    "        x2 = self.maxpool(x1res)\n",
    "\n",
    "        x3 = self.encblock1(x2)\n",
    "        x2res = self.relu(x3 + self.res2(x2))\n",
    "        x4 = self.maxpool(x2res)\n",
    "        \n",
    "        x5 = self.encblock2(x4)\n",
    "        x3res = self.relu(x5 + self.res3(x4))\n",
    "        x6 = self.maxpool(x3res)\n",
    "        \n",
    "        x7 = self.encblock3(x6)\n",
    "        x4res = self.relu(x7 + self.res4(x6))\n",
    "        x8 = self.maxpool(x4res)\n",
    "\n",
    "        x9 = self.mid(x8)\n",
    "        x5res = self.relu(x9 + self.res5(x8))\n",
    "        x10 = self.transpose1(torch.cat([x5res, x8], 1))\n",
    "        \n",
    "        x11 = self.decblock1(x10)\n",
    "        x6res = self.relu(x11 + self.res6(x10))\n",
    "        x12 = self.transpose2(torch.cat([x6res, x6], 1))\n",
    "        \n",
    "        x13 = self.decblock2(x12)\n",
    "        x7res = self.relu(x13 + self.res7(x12))\n",
    "        x14 = self.transpose3(torch.cat([x7res, x4], 1))\n",
    "        \n",
    "        x15 = self.decblock3(x14)\n",
    "        x8res = self.relu(x15 + self.res8(x14))\n",
    "        x16 = self.transpose4(torch.cat([x8res, x2], 1))\n",
    "        \n",
    "        x17 = self.end(x16)\n",
    "        x9res = self.relu(x17 + self.res9(x16))\n",
    "        out = self.sigmoid(x9res)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet()\n",
    "summary = summary(model, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 1e-3\n",
    "lr_decay = 0.1\n",
    "lr_decay_iter = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lr_finder import LearningRateFinder \n",
    "lrf = LearningRateFinder(model, criterion, optimizer)\n",
    "lrf.fit(training_dataloader)\n",
    "lrf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    \"\"\"\n",
    "    Train a model using different learning rates within a range to find the optimal learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer,\n",
    "                 device\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_history = {}\n",
    "        self._model_init = model.state_dict()\n",
    "        self._opt_init = optimizer.state_dict()\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self,\n",
    "            data_loader: torch.utils.data.DataLoader,\n",
    "            steps=100,\n",
    "            min_lr=1e-7,\n",
    "            max_lr=1,\n",
    "            constant_increment=False\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Trains the model for number of steps using varied learning rate and store the statistics\n",
    "        \"\"\"\n",
    "        self.loss_history = {}\n",
    "        self.model.train()\n",
    "        current_lr = min_lr\n",
    "        steps_counter = 0\n",
    "        epochs = math.ceil(steps / len(data_loader))\n",
    "\n",
    "        progressbar = trange(epochs, desc='Progress')\n",
    "        for epoch in progressbar:\n",
    "            batch_iter = tqdm(enumerate(data_loader), 'Training', total=len(data_loader),\n",
    "                              leave=False)\n",
    "\n",
    "            for i, (x, y) in batch_iter:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(x)\n",
    "                loss = self.criterion(out, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.loss_history[current_lr] = loss.item()\n",
    "\n",
    "                steps_counter += 1\n",
    "                if steps_counter > steps:\n",
    "                    break\n",
    "\n",
    "                if constant_increment:\n",
    "                    current_lr += (max_lr - min_lr) / steps\n",
    "                else:\n",
    "                    current_lr = current_lr * (max_lr / min_lr) ** (1 / steps)\n",
    "\n",
    "    def plot(self,\n",
    "             smoothing=True,\n",
    "             clipping=True,\n",
    "             smoothing_factor=0.1\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Shows loss vs learning rate(log scale) in a matplotlib plot\n",
    "        \"\"\"\n",
    "        loss_data = pd.Series(list(self.loss_history.values()))\n",
    "        lr_list = list(self.loss_history.keys())\n",
    "        if smoothing:\n",
    "            loss_data = loss_data.ewm(alpha=smoothing_factor).mean()\n",
    "            loss_data = loss_data.divide(pd.Series(\n",
    "                [1 - (1.0 - smoothing_factor) ** i for i in range(1, loss_data.shape[0] + 1)]))  # bias correction\n",
    "        if clipping:\n",
    "            loss_data = loss_data[10:-5]\n",
    "            lr_list = lr_list[10:-5]\n",
    "        plt.plot(lr_list, loss_data)\n",
    "        plt.xscale('log')\n",
    "        plt.title('Loss vs Learning rate')\n",
    "        plt.xlabel('Learning rate (log scale)')\n",
    "        plt.ylabel('Loss (exponential moving average)')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LearningRateFinder(model, criterion, optimizer, device)\n",
    "lrf.fit(training_dataloader)\n",
    "lrf.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strt2ftprnt",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
